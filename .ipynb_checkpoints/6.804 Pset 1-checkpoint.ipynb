{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.804 Pset 1, Janice Yang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d)\n",
    "i) The hypothesis space does not consider the fact that people have different senses of what is a more \"random\" sequence than the other, even if the number of heads and tails are the exact same. \n",
    "\n",
    "ii) For instance, people are known to think that a sequence like `TTHTH` are more random and thus indicate a fair coin, more than `HHTTT`, even though both of these sequences have the exact same number of heads and tails. Another example is `HTHTH` seeming much less random to a human, and thus less fair than `HTHHT`, again with the exact same number of heads and tails. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a)\n",
    "Let's call hypothesis \"All Multiples of 10\" `H_A`, and \"All Even Numbers\" `H_B`. Assuming those are the only two potential hypotheses, then their priors: `P(H_A) = P(H_B) = 1/2`. \n",
    "\n",
    "\n",
    "Some other calculations:\n",
    "\n",
    "`P(y|H_A)*P(H_A) = (1/10)^3*1/2 = 1/2000`\n",
    "\n",
    "`P(y|H_B)*P(H_B) = (1/50)^3*1/2 = 1/250000`\n",
    "\n",
    "\n",
    "Posterior probability of \"All Multiples of 10\":\n",
    "`P(H_A|y) = 1/2000 / (1/2000+1/250000) = 99.2%`\n",
    "\n",
    "\n",
    "Posterior probability of \"All Even Numbers\":\n",
    "`P(H_B|y) = 1/250000 / (1/2000+1/250000) = 0.8%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b)\n",
    "Since 40 belongs in both `H_A` and `H_B`, if these are the only two hypotheses available, the probability that 40 is in the set is 100%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part e)\n",
    "i) The number game seems to capture and explain Marr's Level 1 as well as Level 2. Through the modelling of hypotheses and their priors and likelihoods, we understand a bit more how our brain works when we give intutions about what hypotheses are more likely or which number is likely to be in the same set. This model is a good approximator for how the logic is carried out between the inputs (seeing the data), and the output (giving an intuitive probability). The actual tree structure of priors and size-based likelihood explains more the algorithmic workings of our cognition.\n",
    "\n",
    "ii) Yes! Although the number game is very specific to just this certain set of numbers, I am assuming that the human brain uses a similar type of decision-making process when judging the likelihood and probability of any other event outside of the lab. We do this when we are deciding what objects are most likely to be in a group together, possible labels for an existing group of items, etc.\n",
    "\n",
    "iii) When people play this game, their hypothesis space comes from their own prior experiences with groups of numbers, either through their education or through everyday life. Thus, depending on one's mathematical background or even environmental conditioning, some people may find certain groups or types of patterns more probable than others (e.g. a mathmetician might be more sensitive to primes, while a first grader may only see that this is a group of odd numbers (with two)!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distribution is given as `P(t_total|t_obs) = P(t_total)*P(t_obs|t_total)/P(t_obs)`. \n",
    "\n",
    "This distribution can be analyzed as a ratio of all the times we see `t_obs` as a result of this specific `t_total`, divided by all the times we see `t_obs` as a result of any `t_total` in the potential space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
